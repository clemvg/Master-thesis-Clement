{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Database/database1/\"\n",
    "usdataset = \"us_equities_news_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalpath = path + usdataset\n",
    "df = pd.read_csv(totalpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['release_date'] make to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'] = df['release_date'].dt.dayofweek\n",
    "df['weekday'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "- filter duplicates and missing values\n",
    "- only tickers nasdaq100 as of Feb 2020\n",
    "- all days\n",
    "- dates: only between 2016-04-17 and 2019-04-16\n",
    "- columns: title, ticker, release date, content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put df in chronological order according to release date\n",
    "df = df.sort_values(by=['release_date'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete content duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the row element where there is missing values in the content column\n",
    "df = df.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of titles that are more than once in df:  6066\n"
     ]
    }
   ],
   "source": [
    "#print number of titles that are more than once in df\n",
    "print(\"Number of titles that are more than once in df: \", len(df[df.duplicated(subset=['title'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of content that are more than once in df:  509\n"
     ]
    }
   ],
   "source": [
    "#print number of content that are more than once in df\n",
    "print(\"Number of content that are more than once in df: \", len(df[df.duplicated(subset=['content'])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220996, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keep only the first occurence of each content\n",
    "df = df.drop_duplicates(subset=['content'], keep='first')\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep only columns title, content, ticker and release date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220996, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keep only columns title, content, ticker and release date\n",
    "df = df[['title', 'content', 'release_date','ticker']]\n",
    "df.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most recurring tickers\n",
    "print(\"Most recurring tickers: \", df['ticker'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the count of elements over the years\n",
    "print(\"Count of elements over the years: \", df['release_date'].dt.year.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OFFICIAL LIST OF NASDAQ 100 TICKERS (source: official website - in 2020)\n",
    "nasdaq100_tickers = ['AAPL', 'ADBE', 'ADI', 'ADSK', 'ALGN', 'ALXN', 'AMAT', 'AMGN', 'AMZN', 'ANSS', 'ASML', 'ATVI', 'ADBE', 'BKNG', 'BIDU', 'BIIB', 'BMRN', 'CDNS', 'CERN', 'CHKP', 'CHTR', 'CMCSA', 'COST', 'CSCO', 'CSX', 'CTAS', 'CTSH', 'DLTR', 'EA', 'EBAY', 'EXPE', 'FAST', 'FB', 'FISV', 'FOXA', 'GILD', 'GOOG', 'GOOGL', 'HAS', 'HSIC', 'IDXX', 'ILMN', 'INCY', 'INTC', 'INTU', 'ISRG', 'JD', 'KDP', 'KHC', 'KLAC', 'LBTYA', 'LBTYK', 'LRCX', 'LULU', 'MAR', 'MCHP', 'MDLZ', 'MELI', 'MNST', 'MSFT', 'MU', 'MXIM', 'MYL', 'NFLX', 'NTES', 'NVDA', 'NXPI', 'ORLY', 'PAYX', 'PCAR', 'PEP', 'PYPL', 'QCOM', 'REGN', 'ROST', 'SBUX', 'SIRI', 'SNPS', 'SPLK', 'SWKS', 'SYMC', 'TMUS', 'TSLA', 'TXN', 'ULTA', 'VRSK', 'VRSN', 'VRTX', 'WBA', 'WDAY', 'WDC', 'XEL', 'XLNX', 'ZM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67741, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the ticker in df that are in nasdaq100_tickers\n",
    "df= df[df['ticker'].isin(nasdaq100_tickers)]\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release date from 2016-04-17 and 2019-04-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41997, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(df['release_date'] > '2016-04-17') & (df['release_date'] < '2019-04-16')]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in the weekends:  3803\n"
     ]
    }
   ],
   "source": [
    "#number of elements in the weekends\n",
    "print(\"Number of elements in the weekends: \", len(df[df['release_date'].dt.weekday > 4]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "52k articles to predict 3 years of weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the average number of elements per date in the weekdays\n",
    "print(\"Average number of elements per date in the weekdays: \", len(df[df['release_date'].dt.weekday < 5])/len(df[df['release_date'].dt.weekday < 5]['release_date'].dt.date.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each holidays/weekend to next date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = \"../Database/price.csv\"\n",
    "df2 = pd.read_csv(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Date'] = pd.to_datetime(df2['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates1 = df2['Date'].dt.date.unique()\n",
    "len(dates1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1070"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "releasedates = df['release_date'].dt.date.unique()\n",
    "len(releasedates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates that in releasedates but not in dates:  {datetime.date(2019, 1, 20), datetime.date(2016, 9, 3), datetime.date(2019, 1, 1), datetime.date(2018, 4, 29), datetime.date(2017, 7, 1), datetime.date(2018, 11, 11), datetime.date(2018, 6, 24), datetime.date(2017, 7, 30), datetime.date(2019, 2, 17), datetime.date(2016, 6, 25), datetime.date(2018, 6, 3), datetime.date(2018, 5, 26), datetime.date(2016, 11, 12), datetime.date(2018, 3, 10), datetime.date(2016, 9, 18), datetime.date(2016, 5, 22), datetime.date(2018, 7, 1), datetime.date(2019, 4, 13), datetime.date(2018, 9, 2), datetime.date(2016, 5, 1), datetime.date(2016, 12, 11), datetime.date(2016, 9, 17), datetime.date(2017, 4, 29), datetime.date(2018, 11, 18), datetime.date(2018, 9, 30), datetime.date(2018, 12, 8), datetime.date(2017, 7, 4), datetime.date(2017, 11, 23), datetime.date(2018, 8, 26), datetime.date(2016, 7, 4), datetime.date(2017, 9, 3), datetime.date(2017, 10, 8), datetime.date(2017, 4, 15), datetime.date(2017, 5, 6), datetime.date(2018, 1, 13), datetime.date(2017, 5, 20), datetime.date(2018, 11, 17), datetime.date(2018, 9, 1), datetime.date(2017, 11, 4), datetime.date(2018, 12, 22), datetime.date(2016, 11, 24), datetime.date(2017, 1, 7), datetime.date(2017, 2, 19), datetime.date(2017, 7, 22), datetime.date(2017, 4, 16), datetime.date(2017, 11, 18), datetime.date(2017, 12, 16), datetime.date(2018, 6, 23), datetime.date(2016, 6, 18), datetime.date(2016, 8, 7), datetime.date(2016, 7, 17), datetime.date(2017, 12, 24), datetime.date(2016, 10, 22), datetime.date(2017, 2, 20), datetime.date(2017, 4, 2), datetime.date(2017, 5, 7), datetime.date(2016, 11, 19), datetime.date(2017, 5, 14), datetime.date(2018, 9, 29), datetime.date(2018, 10, 14), datetime.date(2016, 12, 10), datetime.date(2017, 12, 25), datetime.date(2018, 12, 2), datetime.date(2017, 9, 23), datetime.date(2018, 2, 10), datetime.date(2018, 1, 14), datetime.date(2019, 3, 9), datetime.date(2019, 1, 21), datetime.date(2019, 2, 24), datetime.date(2017, 3, 4), datetime.date(2018, 11, 22), datetime.date(2018, 12, 5), datetime.date(2019, 1, 19), datetime.date(2016, 10, 29), datetime.date(2017, 12, 10), datetime.date(2018, 10, 28), datetime.date(2018, 7, 22), datetime.date(2017, 9, 9), datetime.date(2016, 6, 19), datetime.date(2018, 12, 9), datetime.date(2018, 7, 15), datetime.date(2019, 3, 17), datetime.date(2017, 6, 25), datetime.date(2018, 5, 12), datetime.date(2016, 8, 13), datetime.date(2017, 10, 15), datetime.date(2017, 4, 22), datetime.date(2018, 1, 27), datetime.date(2016, 7, 10), datetime.date(2018, 3, 25), datetime.date(2017, 8, 19), datetime.date(2017, 7, 8), datetime.date(2018, 3, 24), datetime.date(2018, 8, 18), datetime.date(2018, 5, 20), datetime.date(2016, 5, 8), datetime.date(2018, 10, 27), datetime.date(2016, 10, 9), datetime.date(2017, 12, 17), datetime.date(2018, 7, 28), datetime.date(2017, 6, 11), datetime.date(2018, 7, 7), datetime.date(2017, 1, 16), datetime.date(2017, 6, 10), datetime.date(2017, 3, 11), datetime.date(2017, 12, 3), datetime.date(2016, 10, 30), datetime.date(2017, 7, 15), datetime.date(2016, 9, 10), datetime.date(2017, 10, 28), datetime.date(2018, 3, 4), datetime.date(2018, 12, 25), datetime.date(2018, 8, 19), datetime.date(2019, 4, 7), datetime.date(2017, 5, 13), datetime.date(2019, 4, 14), datetime.date(2016, 11, 20), datetime.date(2019, 2, 16), datetime.date(2016, 5, 15), datetime.date(2017, 9, 17), datetime.date(2019, 3, 2), datetime.date(2017, 4, 1), datetime.date(2018, 4, 14), datetime.date(2017, 3, 19), datetime.date(2017, 1, 29), datetime.date(2018, 1, 15), datetime.date(2016, 7, 2), datetime.date(2017, 7, 2), datetime.date(2018, 12, 16), datetime.date(2018, 9, 16), datetime.date(2019, 3, 10), datetime.date(2017, 10, 29), datetime.date(2016, 10, 1), datetime.date(2017, 8, 6), datetime.date(2017, 10, 1), datetime.date(2019, 2, 3), datetime.date(2018, 10, 6), datetime.date(2018, 9, 23), datetime.date(2016, 11, 5), datetime.date(2016, 10, 16), datetime.date(2018, 4, 22), datetime.date(2017, 3, 5), datetime.date(2018, 5, 13), datetime.date(2019, 1, 5), datetime.date(2017, 2, 12), datetime.date(2019, 1, 13), datetime.date(2019, 3, 31), datetime.date(2018, 6, 10), datetime.date(2018, 2, 4), datetime.date(2018, 7, 14), datetime.date(2017, 1, 15), datetime.date(2017, 2, 18), datetime.date(2018, 9, 15), datetime.date(2018, 12, 15), datetime.date(2018, 3, 18), datetime.date(2018, 12, 23), datetime.date(2016, 9, 25), datetime.date(2018, 5, 6), datetime.date(2017, 10, 21), datetime.date(2018, 1, 21), datetime.date(2016, 12, 3), datetime.date(2019, 1, 26), datetime.date(2019, 3, 24), datetime.date(2019, 2, 23), datetime.date(2016, 12, 25), datetime.date(2017, 11, 11), datetime.date(2016, 9, 4), datetime.date(2016, 8, 21), datetime.date(2018, 1, 1), datetime.date(2018, 7, 4), datetime.date(2018, 2, 19), datetime.date(2018, 4, 8), datetime.date(2016, 6, 5), datetime.date(2016, 7, 3), datetime.date(2018, 9, 9), datetime.date(2017, 10, 22), datetime.date(2018, 2, 18), datetime.date(2016, 9, 5), datetime.date(2018, 6, 16), datetime.date(2019, 3, 3), datetime.date(2017, 7, 9), datetime.date(2018, 2, 24), datetime.date(2017, 7, 29), datetime.date(2018, 7, 8), datetime.date(2017, 11, 19), datetime.date(2018, 6, 17), datetime.date(2018, 5, 5), datetime.date(2017, 3, 26), datetime.date(2018, 8, 4), datetime.date(2018, 2, 25), datetime.date(2017, 5, 21), datetime.date(2017, 9, 24), datetime.date(2016, 11, 27), datetime.date(2016, 7, 31), datetime.date(2016, 6, 12), datetime.date(2017, 8, 12), datetime.date(2017, 4, 9), datetime.date(2016, 12, 26), datetime.date(2018, 8, 12), datetime.date(2018, 11, 3), datetime.date(2018, 4, 15), datetime.date(2018, 6, 2), datetime.date(2019, 2, 10), datetime.date(2016, 9, 11), datetime.date(2017, 6, 18), datetime.date(2018, 5, 19), datetime.date(2017, 4, 30), datetime.date(2018, 3, 30), datetime.date(2017, 2, 4), datetime.date(2017, 12, 2), datetime.date(2017, 4, 23), datetime.date(2018, 12, 30), datetime.date(2019, 3, 16), datetime.date(2017, 2, 25), datetime.date(2016, 6, 11), datetime.date(2017, 11, 12), datetime.date(2017, 1, 28), datetime.date(2016, 7, 16), datetime.date(2017, 5, 29), datetime.date(2017, 1, 1), datetime.date(2018, 8, 11), datetime.date(2017, 8, 5), datetime.date(2017, 1, 8), datetime.date(2016, 5, 7), datetime.date(2018, 11, 25), datetime.date(2018, 1, 20), datetime.date(2016, 4, 23), datetime.date(2017, 9, 2), datetime.date(2017, 8, 20), datetime.date(2017, 9, 30), datetime.date(2016, 12, 24), datetime.date(2017, 3, 25), datetime.date(2017, 12, 30), datetime.date(2018, 6, 30), datetime.date(2019, 4, 6), datetime.date(2016, 8, 14), datetime.date(2018, 3, 3), datetime.date(2017, 11, 26), datetime.date(2016, 4, 30), datetime.date(2018, 10, 7), datetime.date(2019, 1, 27), datetime.date(2017, 1, 22), datetime.date(2018, 8, 25), datetime.date(2018, 1, 7), datetime.date(2017, 10, 14), datetime.date(2018, 1, 28), datetime.date(2019, 3, 30), datetime.date(2016, 11, 13), datetime.date(2017, 8, 13), datetime.date(2016, 11, 6), datetime.date(2018, 4, 7), datetime.date(2018, 7, 29), datetime.date(2016, 10, 23), datetime.date(2017, 7, 23), datetime.date(2018, 1, 6), datetime.date(2016, 7, 24), datetime.date(2019, 1, 12), datetime.date(2017, 3, 12), datetime.date(2017, 2, 26), datetime.date(2017, 1, 21), datetime.date(2018, 4, 28), datetime.date(2018, 12, 29), datetime.date(2017, 12, 23), datetime.date(2017, 11, 5), datetime.date(2019, 2, 9), datetime.date(2017, 8, 27), datetime.date(2018, 5, 28), datetime.date(2016, 5, 30), datetime.date(2017, 1, 2), datetime.date(2019, 3, 23), datetime.date(2016, 12, 18), datetime.date(2018, 9, 8), datetime.date(2018, 4, 1), datetime.date(2016, 7, 23), datetime.date(2017, 9, 10), datetime.date(2018, 4, 21), datetime.date(2017, 4, 8), datetime.date(2018, 12, 1), datetime.date(2017, 9, 16), datetime.date(2017, 12, 31), datetime.date(2018, 2, 17), datetime.date(2019, 2, 2), datetime.date(2018, 10, 21), datetime.date(2017, 7, 16), datetime.date(2017, 12, 9), datetime.date(2018, 10, 13), datetime.date(2018, 10, 20), datetime.date(2017, 5, 28), datetime.date(2018, 11, 4), datetime.date(2018, 8, 5), datetime.date(2016, 5, 29), datetime.date(2017, 9, 4), datetime.date(2018, 3, 11), datetime.date(2017, 2, 5), datetime.date(2016, 12, 4), datetime.date(2017, 6, 4), datetime.date(2016, 8, 28), datetime.date(2018, 9, 3), datetime.date(2017, 5, 27), datetime.date(2018, 2, 11), datetime.date(2016, 6, 26), datetime.date(2017, 6, 24), datetime.date(2018, 9, 22), datetime.date(2016, 4, 24), datetime.date(2016, 10, 2), datetime.date(2018, 3, 31), datetime.date(2017, 4, 14), datetime.date(2018, 11, 24), datetime.date(2018, 5, 27), datetime.date(2017, 3, 18), datetime.date(2018, 3, 17), datetime.date(2018, 2, 3), datetime.date(2017, 6, 3), datetime.date(2017, 11, 25), datetime.date(2019, 1, 6), datetime.date(2019, 2, 18)}\n",
      "Length of dates that in releasedates but not in dates:  316\n"
     ]
    }
   ],
   "source": [
    "#the dates that in releasedates but not in dates and its length\n",
    "diff =  set(releasedates) - set(dates1)\n",
    "print(\"Dates that in releasedates but not in dates: \",diff)\n",
    "print(\"Length of dates that in releasedates but not in dates: \", len(diff))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the dates in df to the next date chronoligically that is in dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign each date in df that is not in variable dates1 to the first following day that is in the dates1\n",
    "for i in range(len(df)):\n",
    "    if df['release_date'].iloc[i].date() not in dates1:\n",
    "        df['release_date'].iloc[i] = df['release_date'].iloc[i] + pd.Timedelta(days=1)\n",
    "        while df['release_date'].iloc[i].date() not in dates1:\n",
    "            df['release_date'].iloc[i] = df['release_date'].iloc[i] + pd.Timedelta(days=1)\n",
    "            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing too long content articles avoid to do it in next code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep first 512 TOKENS in content\n",
    "df['content'] = df['content'].str[:512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show per year the number of elements\n",
    "df['release_date'].dt.year.value_counts().sort_index().plot(kind='bar')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>release_date</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stocks Poised For More Upside</td>\n",
       "      <td>In my experience  there is only one motivatio...</td>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U S  futures point to lower open amid dropping...</td>\n",
       "      <td>Investing com   Wall Street futures pointed to...</td>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>NFLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netflix shares plunge as subscriber forecasts ...</td>\n",
       "      <td>By Anya George Tharakan and Lisa Richwine  Reu...</td>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>NFLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dow reclaims 18 000 as quarterly scorecards st...</td>\n",
       "      <td>By Noel Randewich  Reuters    The Dow Jones in...</td>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>NFLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netherlands stocks higher at close of trade  A...</td>\n",
       "      <td>Investing com   Netherlands stocks were higher...</td>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>ASML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41992</th>\n",
       "      <td>Netherlands stocks higher at close of trade  A...</td>\n",
       "      <td>Investing com   Netherlands stocks were higher...</td>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>ASML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41993</th>\n",
       "      <td>Analysts Estimate Hasbro  HAS  To Report A Dec...</td>\n",
       "      <td>Wall Street expects a year over year decline i...</td>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>HAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41994</th>\n",
       "      <td>Apple  allies seek billions in U S  trial test...</td>\n",
       "      <td>By Stephen Nellis  Reuters    Apple Inc  NASDA...</td>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>INTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>Sweden s Veoneer bets driverless car delay wil...</td>\n",
       "      <td>By Johannes Hellstrom and Esha Vaish STOCKHOLM...</td>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>INTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>Exclusive  Toyota sells electric vehicle techn...</td>\n",
       "      <td>By Norihiko Shirouzu BEIJING  Reuters    Toyot...</td>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>INTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41997 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0                          Stocks Poised For More Upside   \n",
       "1      U S  futures point to lower open amid dropping...   \n",
       "2      Netflix shares plunge as subscriber forecasts ...   \n",
       "3      Dow reclaims 18 000 as quarterly scorecards st...   \n",
       "4      Netherlands stocks higher at close of trade  A...   \n",
       "...                                                  ...   \n",
       "41992  Netherlands stocks higher at close of trade  A...   \n",
       "41993  Analysts Estimate Hasbro  HAS  To Report A Dec...   \n",
       "41994  Apple  allies seek billions in U S  trial test...   \n",
       "41995  Sweden s Veoneer bets driverless car delay wil...   \n",
       "41996  Exclusive  Toyota sells electric vehicle techn...   \n",
       "\n",
       "                                                 content release_date ticker  \n",
       "0       In my experience  there is only one motivatio...   2016-04-18   TSLA  \n",
       "1      Investing com   Wall Street futures pointed to...   2016-04-18   NFLX  \n",
       "2      By Anya George Tharakan and Lisa Richwine  Reu...   2016-04-18   NFLX  \n",
       "3      By Noel Randewich  Reuters    The Dow Jones in...   2016-04-18   NFLX  \n",
       "4      Investing com   Netherlands stocks were higher...   2016-04-18   ASML  \n",
       "...                                                  ...          ...    ...  \n",
       "41992  Investing com   Netherlands stocks were higher...   2019-04-15   ASML  \n",
       "41993  Wall Street expects a year over year decline i...   2019-04-15    HAS  \n",
       "41994  By Stephen Nellis  Reuters    Apple Inc  NASDA...   2019-04-15   INTC  \n",
       "41995  By Johannes Hellstrom and Esha Vaish STOCKHOLM...   2019-04-15   INTC  \n",
       "41996  By Norihiko Shirouzu BEIJING  Reuters    Toyot...   2019-04-15   INTC  \n",
       "\n",
       "[41997 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"news_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ffd396e69a736e14539511bd5b9588714e184f550601071749c2b1d78c18962"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
