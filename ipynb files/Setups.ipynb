{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pllh1OeqbMrk"
      },
      "source": [
        "# Loading packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pylP9Zoi1fP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-pmbcuDIGYP"
      },
      "outputs": [],
      "source": [
        "#preprocessing LSTM (train-test split, minmaxscaler, validation_set_criteria)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q67aut9WjBu_"
      },
      "outputs": [],
      "source": [
        "#For LSTM architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout #all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUjMvFmEqkxH"
      },
      "outputs": [],
      "source": [
        "#for stationarity tests\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "#for K-fold cross validation\n",
        "from sklearn.model_selection import TimeSeriesSplit \n",
        "\n",
        "#for output metrics\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dioea8EsNCCD"
      },
      "outputs": [],
      "source": [
        "#needed for Emprirical mode decomposition\n",
        "! pip install EMD-signal\n",
        "from PyEMD import EMD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZktVSvnUEV5"
      },
      "outputs": [],
      "source": [
        "#for hypertuning\n",
        "from sklearn.model_selection import ParameterGrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27BXjwaSTUS7"
      },
      "outputs": [],
      "source": [
        "#model seed with prime numbers\n",
        "import tensorflow as tf\n",
        "import random as python_random\n",
        "\n",
        "def reset_seeds():\n",
        "   #prime number\n",
        "   #pas it√©rer\n",
        "   np.random.seed(113)\n",
        "   python_random.seed(113)\n",
        "   tf.random.set_seed(1151)\n",
        "\n",
        "# reset_seeds() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKFa13AOSx5I"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVax9R9cXCaE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOJVz2mHWsm6"
      },
      "outputs": [],
      "source": [
        "# Load the numpy array from the file \"my_array.npy\"\n",
        "adj_close = np.load(\"/content/drive/MyDrive/adj_close.npy\")\n",
        "sentiment_data = np.load(\"/content/drive/MyDrive/sentiment_data.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ-Zm5Vcqunr"
      },
      "source": [
        "# **Mode decomposition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Heo3ay-5jd8O"
      },
      "outputs": [],
      "source": [
        "idx = np.arange(len(sentiment_data)) # for the time dimension below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qiBej1k-RSr"
      },
      "outputs": [],
      "source": [
        "#Two decomposition techniques. WIll test later if one is leading to better results or not. EMD for the moment is by default and more than enough\n",
        "Signal = adj_close\n",
        "T = idx\n",
        "emd = EMD()\n",
        "IMFs = emd(Signal)\n",
        "nIMFs = len(IMFs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3Fy7TybOXe0"
      },
      "outputs": [],
      "source": [
        "# Plotting the decomposition (visualise the subsequences getting from the original signal)\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.subplot(nIMFs+1, 1, 1)\n",
        "plt.plot(T, Signal, 'r')\n",
        "\n",
        "for n in range(nIMFs):\n",
        "  plt.subplot(nIMFs+1, 1, n+2)\n",
        "  plt.plot(T, IMFs[n], 'g')\n",
        "  plt.ylabel(\"IMF %i\" %(n+1))\n",
        "  plt.locator_params(axis='y', nbins=5)\n",
        "\n",
        "plt.xlabel(\"Time\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Stacking IMFs in one single LSTM"
      ],
      "metadata": {
        "id": "YgD9OktdQkYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters to search over\n",
        "\n",
        "#only for table\n",
        "set_up = 3\n",
        "\n",
        "# not tuning\n",
        "test_size_split = 0.2\n",
        "\n",
        "#hyper-parameters\n",
        "params = {\n",
        "    'batch_size': [8, 16],\n",
        "    'num_layers': [1, 2],\n",
        "    'neurons': [32, 64],\n",
        "    'epochs': [300], #[100, 200, 300],\n",
        "    'nmbr_folds': [5,10],\n",
        "    'news_using': ['yes','no'],\n",
        "    'lookback_period': [15,30,60]\n",
        "    # 'attention_layer': ['yes', 'no'] #\n",
        "}\n",
        "\n",
        "# Generate the grid of hyperparameters\n",
        "param_grid = list(ParameterGrid(params))\n",
        "print('Number of combinations:', len(param_grid))"
      ],
      "metadata": {
        "id": "S4y_LmVJRCwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Using different LSTM models for each IMFs\n"
      ],
      "metadata": {
        "id": "cf-TirAqQs1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters to search over\n",
        "\n",
        "#only for table\n",
        "set_up = 2\n",
        "\n",
        "#set-up 2 takes more time which is logic\n",
        "\n",
        "# not tuning (limitation)\n",
        "test_size_split = 0.2\n",
        "\n",
        "#hyper-parameters\n",
        "params = {\n",
        "    'batch_size': [8, 16],\n",
        "    'num_layers': [1, 2],\n",
        "    'neurons': [32, 64],\n",
        "    'epochs': [300], #[100, 200, 300],\n",
        "    'nmbr_folds': [5,10],\n",
        "    'news_using': ['yes','no'],\n",
        "        'lookback_period': [15,30,60]\n",
        "    # 'attention_layer': ['yes', 'no'] #no need put some numbers\n",
        "}\n",
        "\n",
        "# Generate the grid of hyperparameters\n",
        "param_grid = list(ParameterGrid(params))\n",
        "print('Number of combinations:', len(param_grid))"
      ],
      "metadata": {
        "id": "QguBOowUQ0S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVbmyLJQRWL2"
      },
      "source": [
        "## Performing LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dyTiDElRWL3"
      },
      "outputs": [],
      "source": [
        "#modified without zero\n",
        "def create_dataset(dataset, look_back):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back):\n",
        "\t\ta = dataset[i:(i+look_back)]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back])\n",
        "\treturn np.array(dataX), np.array(dataY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru6lCEarRWL4"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model\n",
        "def create_model(neurons, num_layers, lookback,dimension):#attention_layer,\n",
        "    model = Sequential()\n",
        "    if num_layers < 2:\n",
        "      model.add(LSTM(neurons, input_shape=(lookback, dimension)))\n",
        "    else: # num_layers > 1:\n",
        "      model.add(LSTM(neurons, input_shape=(lookback, dimension), return_sequences=True))\n",
        "      model.add(LSTM(neurons))\n",
        "    # if attention_layer == 'yes':\n",
        "    #   print('attention')\n",
        "      # model.add(Attention())\n",
        "    model.add(Dropout(0.1))     \n",
        "\n",
        "    model.add(Dense(1)) \n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUG73fwiRWL6"
      },
      "outputs": [],
      "source": [
        "column_names = ['News', 'RMSE', 'MAPE', 'MAE', 'R2', 'Batch_size', 'Initiated_epochs', 'Lookback', 'Num_layers', 'Num_neurons', 'Test_size_split', 'Num_folds', 'Set_up']\n",
        "results = pd.DataFrame(columns=column_names)\n",
        "\n",
        "for idx, param_set in enumerate(param_grid):\n",
        "    print(f\"Printing numbers for param_set #{idx+1}:\")\n",
        "    news_using = param_set['news_using']\n",
        "    batch_size = param_set['batch_size']\n",
        "    num_layers = param_set['num_layers']\n",
        "    neurons = param_set['neurons']\n",
        "    epochs = param_set['epochs']\n",
        "    nmbr_folds = param_set['nmbr_folds']\n",
        "    lookback_period = param_set['lookback_period']\n",
        "    # attention_layer = param_set['attention_layer']\n",
        "    print(param_set)\n",
        "\n",
        "    ## Historical price [Y]\n",
        "\n",
        "    # 1) Reshape price, another name variable not to influence stacking dimensions below\n",
        "    #maybe as float 32\n",
        "    reshaped_price = np.reshape(adj_close, (-1, 1))\n",
        "    # 2) Train-test split - I split before Minmax scaling! GREAT\n",
        "    train_data, test_data = train_test_split(reshaped_price, test_size=test_size_split, shuffle=False)\n",
        "    # 3) Normalize data\n",
        "    scaler = MinMaxScaler()\n",
        "    train_data = scaler.fit_transform(train_data)\n",
        "    test_data = scaler.transform(test_data)\n",
        "    # 4) lookback_period dataset\n",
        "    train_X, train_Y = create_dataset(train_data, lookback_period)\n",
        "    test_X, test_Y = create_dataset(test_data, lookback_period) \n",
        "\n",
        "    ##Stacked data [X]\n",
        "\n",
        "    # scaler here more correct and does not change anything\n",
        "    scaler_sentiment = MinMaxScaler()\n",
        "    sentiment_data_scaled = sentiment_data.reshape(-1, 1)\n",
        "    sentiment_data_scaled = scaler_sentiment.fit_transform(sentiment_data_scaled)\n",
        "\n",
        "    scaler_price = MinMaxScaler()\n",
        "    adj_close_scaled = adj_close.reshape(-1, 1)\n",
        "    adj_close_scaled = scaler_price.fit_transform(adj_close_scaled)\n",
        "\n",
        "    # 1) stack data\n",
        "    if news_using == 'yes':\n",
        "      data_stacked = np.column_stack([adj_close_scaled,sentiment_data_scaled])\n",
        "      second_dimension = data_stacked.shape[1]\n",
        "    else:\n",
        "      data_stacked = np.column_stack([adj_close_scaled])\n",
        "      second_dimension = data_stacked.shape[1]  \n",
        "\n",
        "    # 2) Train-test split\n",
        "    train_stacked, test_stacked = train_test_split(data_stacked, test_size=test_size_split, shuffle=False)\n",
        "\n",
        "    # 4) Create lookback_period dataset\n",
        "    train_stacked_X, train_stacked_Y = create_dataset(train_stacked, lookback_period)\n",
        "    test_stacked_X, test_stacked_Y = create_dataset(test_stacked, lookback_period)\n",
        "\n",
        "    # Define your LSTM model\n",
        "    reset_seeds()\n",
        "    model = create_model(neurons=neurons, num_layers=num_layers, lookback=lookback_period, dimension = second_dimension) #attention_layer=attention_layer,\n",
        "\n",
        "    # Define your TimeSeriesSplit object\n",
        "    tscv = TimeSeriesSplit(n_splits=nmbr_folds)\n",
        "\n",
        "    # Loop through the splits and fit the model\n",
        "    for train_index, test_index in tscv.split(train_stacked_X):\n",
        "        \n",
        "        # Split the data into train and test sets based on the TimeSeriesSplit\n",
        "        train_stacked_X_fold, test_stacked_X_fold = train_stacked_X[train_index], train_stacked_X[test_index]\n",
        "        train_Y_fold, test_Y_fold = train_Y[train_index], train_Y[test_index]\n",
        "        \n",
        "        # fit the model\n",
        "        #model.summary\n",
        "        earlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=2, mode='min') \n",
        "        model.fit(train_stacked_X_fold, train_Y_fold, epochs=epochs, batch_size=batch_size, verbose=0, validation_split=0.1, callbacks=[earlystop])\n",
        "\n",
        "    #predictions model on test set \n",
        "    test_set_predictions = model.predict(test_stacked_X)\n",
        "    unscaled_test_set_predictions = scaler.inverse_transform(test_set_predictions)\n",
        "    unscaled_test_Y = scaler.inverse_transform(test_Y)\n",
        "\n",
        "    #metrics on test set\n",
        "    RMSE = math.sqrt(mean_squared_error(unscaled_test_Y, unscaled_test_set_predictions))\n",
        "    MAE = mean_absolute_error(unscaled_test_Y, unscaled_test_set_predictions)\n",
        "    price_unscaled_test_Y_arr = np.array(unscaled_test_Y)\n",
        "    unscaled_test_set_predictions_arr = np.array(unscaled_test_set_predictions)\n",
        "    MAPE = np.mean(np.abs((price_unscaled_test_Y_arr - unscaled_test_set_predictions_arr) / price_unscaled_test_Y_arr)) * 100\n",
        "    R2 = r2_score(unscaled_test_Y, unscaled_test_set_predictions)\n",
        "\n",
        "    print(\"RMSE: {}, MAE: {}, MAPE: {}, R2: {}\".format(RMSE, MAE, MAPE, R2))\n",
        "\n",
        "    row_data = {\n",
        "    'News': news_using,\n",
        "    'RMSE': RMSE,\n",
        "    'MAPE': MAPE,\n",
        "    'MAE': MAE,\n",
        "    'R2': R2,\n",
        "    'Batch_size': batch_size,\n",
        "    'Initiated_epochs': epochs,\n",
        "    'Lookback': lookback_period,\n",
        "    'Num_layers': num_layers,\n",
        "    'Num_neurons': neurons,\n",
        "    'Test_size_split': test_size_split,\n",
        "    'Num_folds': nmbr_folds,\n",
        "    'Set_up': set_up,\n",
        "    }\n",
        "    new_row = pd.DataFrame(row_data, index=[0])\n",
        "    results = pd.concat([results, new_row], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save df\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "results.to_csv('/content/drive/My Drive/Colab Notebooks/Official/Excel/resultsoutput_2.csv', index=False)"
      ],
      "metadata": {
        "id": "n6dv181FRWMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QKch8qRRWMV"
      },
      "outputs": [],
      "source": [
        "# Set the figure size and DPI\n",
        "fig, ax = plt.subplots(figsize=(14, 11), dpi=80)\n",
        "\n",
        "# Generate the plot lines\n",
        "x = np.linspace(1, len(unscaled_test_set_predictions) + 1, len(unscaled_test_set_predictions), endpoint=True)\n",
        "predicted_line, = ax.plot(x, unscaled_test_set_predictions, label=\"Predicted Value\")\n",
        "actual_line, = ax.plot(x, unscaled_test_Y, label=\"Actual Value\") # or test_Y_fold\n",
        "\n",
        "# Set the title and axis labels\n",
        "ax.set_title('Prediction vs. Actual Values')\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Value')\n",
        "\n",
        "# Add the legend\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing LSTM"
      ],
      "metadata": {
        "id": "JnYZNeZYSh1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install attention"
      ],
      "metadata": {
        "id": "bfzFYDA_KDTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modified without zero\n",
        "def create_dataset(dataset, look_back):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back):\n",
        "\t\ta = dataset[i:(i+look_back)]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back])\n",
        "\treturn np.array(dataX), np.array(dataY)"
      ],
      "metadata": {
        "id": "wFugRGPZSh1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "def create_model(neurons, num_layers, lookback,dimension):#attention_layer,\n",
        "    model = Sequential()\n",
        "    if num_layers < 2:\n",
        "      model.add(LSTM(neurons, input_shape=(lookback, dimension)))\n",
        "    else: # num_layers > 1:\n",
        "      model.add(LSTM(neurons, input_shape=(lookback, dimension), return_sequences=True))\n",
        "      model.add(LSTM(neurons))\n",
        "    # if attention_layer == 'yes':\n",
        "    #   print('attention')\n",
        "      # model.add(Attention()) #PROBLEM\n",
        "    model.add(Dropout(0.1))     \n",
        "\n",
        "    model.add(Dense(1)) #know which functions are used\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "J64fqpxyAoxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['News', 'RMSE', 'MAPE', 'MAE', 'R2', 'Batch_size', 'Initiated_epochs', 'Lookback', 'Num_layers', 'Num_neurons', 'Test_size_split', 'Num_folds', 'Set_up']\n",
        "results = pd.DataFrame(columns=column_names)\n",
        "\n",
        "for param_set in param_grid:\n",
        "    news_using = param_set['news_using']\n",
        "    batch_size = param_set['batch_size']\n",
        "    num_layers = param_set['num_layers']\n",
        "    neurons = param_set['neurons']\n",
        "    epochs = param_set['epochs']\n",
        "    nmbr_folds = param_set['nmbr_folds']\n",
        "    lookback_period = param_set['lookback_period']\n",
        "    # attention_layer = param_set['attention_layer']\n",
        "\n",
        "    print(param_set)\n",
        "    reset_seeds() \n",
        "    ## Historical adj_close\n",
        "\n",
        "    # 1) Reshape adj_close [Y]\n",
        "    #maybe as float 32\n",
        "    reshaped_adj_close = np.reshape(adj_close, (-1, 1))\n",
        "    # 2) Train-test split - I split before Minmax scaling! GREAT\n",
        "    train_data, test_data = train_test_split(reshaped_adj_close, test_size=test_size_split, shuffle=False)\n",
        "    # 3) Normalize data\n",
        "    scaler = MinMaxScaler()\n",
        "    train_data = scaler.fit_transform(train_data)\n",
        "    test_data = scaler.transform(test_data)\n",
        "    # 4) lookback_period dataset\n",
        "    train_X, train_Y = create_dataset(train_data, lookback_period)\n",
        "    test_X, test_Y = create_dataset(test_data, lookback_period) \n",
        "\n",
        "    ## Stacked data [X]\n",
        "\n",
        "    ##1) scale before splitting but is fine because minmax scaler is used apprpriately\n",
        "\n",
        "    scaler1 = MinMaxScaler()\n",
        "    scaler2 = MinMaxScaler()\n",
        "    scaler3 = MinMaxScaler()\n",
        "    scaler4 = MinMaxScaler()\n",
        "    scaler5 = MinMaxScaler()\n",
        "    scaler6 = MinMaxScaler()\n",
        "\n",
        "    a = IMFs[0].reshape(-1, 1)\n",
        "    b = IMFs[1].reshape(-1, 1)\n",
        "    c = IMFs[2].reshape(-1, 1)\n",
        "    d = IMFs[3].reshape(-1, 1)\n",
        "    e = IMFs[4].reshape(-1, 1)\n",
        "    f = IMFs[5].reshape(-1, 1)\n",
        "\n",
        "    a = scaler1.fit_transform(a)\n",
        "    b = scaler2.fit_transform(b)\n",
        "    c = scaler3.fit_transform(c)\n",
        "    d = scaler4.fit_transform(d)\n",
        "    e = scaler5.fit_transform(e)\n",
        "    f = scaler6.fit_transform(f)\n",
        "\n",
        "    scaler_sentiment = MinMaxScaler()\n",
        "    sentiment_data_scaled = sentiment_data.reshape(-1, 1)\n",
        "    sentiment_data_scaled = scaler_sentiment.fit_transform(sentiment_data_scaled)\n",
        "\n",
        "    # 1) stack data\n",
        "    IMFs_stacked = np.column_stack((a,b,c,d,e,f))\n",
        "\n",
        "    if news_using == 'yes':\n",
        "      data_stacked = np.column_stack([IMFs_stacked,sentiment_data_scaled])\n",
        "      second_dimension = data_stacked.shape[1]\n",
        "    else:\n",
        "      data_stacked = IMFs_stacked\n",
        "      second_dimension = data_stacked.shape[1]\n",
        "\n",
        "    # 2) Train-test split\n",
        "    train_stacked, test_stacked = train_test_split(data_stacked, test_size=test_size_split, shuffle=False)\n",
        "\n",
        "    # 4) Create lookback_period dataset\n",
        "    train_stacked_X, train_stacked_Y = create_dataset(train_stacked, lookback_period)\n",
        "    test_stacked_X, test_stacked_Y = create_dataset(test_stacked, lookback_period)\n",
        "\n",
        "    #Create model and compile it (function)  \n",
        "    model = create_model(neurons=neurons, num_layers=num_layers, lookback=lookback_period, dimension = second_dimension) #, attention_layer=attention_layer\n",
        "\n",
        "    # Define your TimeSeriesSplit object\n",
        "    tscv = TimeSeriesSplit(n_splits=nmbr_folds)\n",
        "\n",
        "    # Define the lists to store the evaluation metrics for each fold\n",
        "    rmse_list, mae_list, mape_list, r2_list = [], [], [], []\n",
        "\n",
        "    # Loop through the splits and fit the model\n",
        "    for train_index, test_index in tscv.split(train_stacked_X):\n",
        "                \n",
        "        # Split the data into train and test sets based on the TimeSeriesSplit\n",
        "        train_stacked_X_fold, test_stacked_X_fold = train_stacked_X[train_index], train_stacked_X[test_index]\n",
        "        train_Y_fold, test_Y_fold = train_Y[train_index], train_Y[test_index]\n",
        "        \n",
        "        #Fit the model\n",
        "        # model.summary()\n",
        "        earlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min') #Depending on Manon, now only 300 epochs\n",
        "        model.fit(train_stacked_X_fold, train_Y_fold, batch_size=batch_size, epochs=epochs, verbose=0, validation_split=0.1, callbacks=[earlystop]) \n",
        "        # model.fit(train_stacked_X_fold, train_Y_fold,batch_size=batch_size, epochs=epochs, verbose=0) \n",
        "        \n",
        "        #make predictions of folds blc\n",
        "        testPredict = model.predict(test_stacked_X_fold)\n",
        "        predictions = scaler.inverse_transform(testPredict)\n",
        "        test_Y_fold = scaler.inverse_transform(test_Y_fold)\n",
        "        # Calculate evaluation metrics and store them in lists\n",
        "        RMSE = math.sqrt(mean_squared_error(test_Y_fold, predictions))\n",
        "        MAE = mean_absolute_error(test_Y_fold, predictions)\n",
        "        price_test_Y_fold_arr = np.array(test_Y_fold)\n",
        "        predictions_arr = np.array(predictions)\n",
        "        MAPE = np.mean(np.abs((price_test_Y_fold_arr - predictions_arr) / price_test_Y_fold_arr)) * 100\n",
        "        R2 = r2_score(test_Y_fold, predictions)\n",
        "        rmse_list.append(RMSE)\n",
        "        mae_list.append(MAE)\n",
        "        mape_list.append(MAPE)\n",
        "        r2_list.append(R2)\n",
        "    \n",
        "    print(\"Average evaluation metrics across all folds:\")\n",
        "    print(\"RMSE: {}, MAE: {}, MAPE: {}, R2: {}\".format(np.mean(rmse_list), np.mean(mae_list), np.mean(mape_list), np.mean(r2_list)))\n",
        "\n",
        "    #predictions model on TEST set \n",
        "    test_set_predictions = model.predict(test_stacked_X)\n",
        "    unscaled_test_set_predictions = scaler.inverse_transform(test_set_predictions)\n",
        "    unscaled_test_Y = scaler.inverse_transform(test_Y)\n",
        "\n",
        "    #metrics on test set\n",
        "    RMSE = math.sqrt(mean_squared_error(unscaled_test_Y, unscaled_test_set_predictions))\n",
        "    MAE = mean_absolute_error(unscaled_test_Y, unscaled_test_set_predictions)\n",
        "    price_unscaled_test_Y_arr = np.array(unscaled_test_Y)\n",
        "    unscaled_test_set_predictions_arr = np.array(unscaled_test_set_predictions)\n",
        "    MAPE = np.mean(np.abs((price_unscaled_test_Y_arr - unscaled_test_set_predictions_arr) / price_unscaled_test_Y_arr)) * 100\n",
        "    R2 = r2_score(unscaled_test_Y, unscaled_test_set_predictions)\n",
        "\n",
        "    print(\"RMSE: {}, MAE: {}, MAPE: {}, R2: {}\".format(RMSE, MAE, MAPE, R2))\n",
        "\n",
        "    row_data = {\n",
        "    'News': news_using,\n",
        "    'RMSE': RMSE,\n",
        "    'MAPE': MAPE,\n",
        "    'MAE': MAE,\n",
        "    'R2': R2,\n",
        "    'Batch_size': batch_size,\n",
        "    'Initiated_epochs': epochs,\n",
        "    'Lookback': lookback_period,\n",
        "    'Num_layers': num_layers,\n",
        "    'Num_neurons': neurons,\n",
        "    'Test_size_split': test_size_split,\n",
        "    'Num_folds': nmbr_folds,\n",
        "    'Set_up': set_up,\n",
        "    }\n",
        "    new_row = pd.DataFrame(row_data, index=[0])\n",
        "    results = pd.concat([results, new_row], ignore_index=True)\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "8uAmbRSfPwEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save df\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "results.to_csv('/content/drive/My Drive/Colab Notebooks/resultsoutput_3.csv', index=False)"
      ],
      "metadata": {
        "id": "VD9NYbUiiMRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size and DPI\n",
        "fig, ax = plt.subplots(figsize=(14, 11), dpi=80)\n",
        "\n",
        "# Plot lines\n",
        "x = np.linspace(1, len(unscaled_test_set_predictions) + 1, len(unscaled_test_set_predictions), endpoint=True)\n",
        "predicted_line, = ax.plot(x, unscaled_test_set_predictions, label=\"Predicted Value\")\n",
        "actual_line, = ax.plot(x, unscaled_test_Y, label=\"Actual Value\") # or test_Y_fold\n",
        "\n",
        "# Title and axis labels\n",
        "ax.set_title('Prediction vs. Actual Values')\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Value')\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "Q1r8C080eBr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6N3R94sS_Sv"
      },
      "source": [
        "# 1. No EMD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANgQfAQcibvv"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters to search over\n",
        "set_up = 1\n",
        "test_size_split = 0.2\n",
        "\n",
        "#hyper-parameters\n",
        "params = {\n",
        "    'batch_size': [8, 16],\n",
        "    'num_layers': [1, 2],\n",
        "    'neurons': [32, 64],\n",
        "    'epochs': [300], #[100, 200, 300],\n",
        "    'nmbr_folds': [5,10],\n",
        "    'news_using': ['yes','no'],\n",
        "    'lookback_period': [15,30,60]\n",
        "    # 'attention_layer': ['yes', 'no'] #no need put some numbers\n",
        "}\n",
        "\n",
        "# Generate the grid of hyperparameters\n",
        "param_grid = list(ParameterGrid(params))\n",
        "print('Number of combinations:', len(param_grid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPPvaQl4UWBh"
      },
      "source": [
        "## Performing LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1HmDtwbUh8v"
      },
      "outputs": [],
      "source": [
        "# pip install attention "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXw8oZCV7FHE"
      },
      "outputs": [],
      "source": [
        "#modified without zero\n",
        "def create_dataset(dataset, look_back):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back):\n",
        "\t\ta = dataset[i:(i+look_back)]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back])\n",
        "\treturn np.array(dataX), np.array(dataY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGzwvfYl4uib"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model\n",
        "def create_model(neurons, num_layers, lookback,dimension):#attention_layer,\n",
        "    model = Sequential()\n",
        "    if num_layers < 2:\n",
        "      model.add(LSTM(neurons, input_shape=(lookback, dimension)))\n",
        "    else: # num_layers > 1:\n",
        "      model.add(LSTM(neurons, input_shape=(lookback, dimension), return_sequences=True))\n",
        "      model.add(LSTM(neurons))\n",
        "    # if attention_layer == 'yes':\n",
        "    #   print('attention')\n",
        "      # model.add(Attention())\n",
        "    model.add(Dropout(0.1))     \n",
        "\n",
        "    model.add(Dense(1)) \n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riW9isOWYg31"
      },
      "outputs": [],
      "source": [
        "column_names = ['News', 'RMSE', 'MAPE', 'MAE', 'R2', 'Batch_size', 'Initiated_epochs', 'Lookback', 'Num_layers', 'Num_neurons', 'Test_size_split', 'Num_folds', 'Set_up']\n",
        "results = pd.DataFrame(columns=column_names)\n",
        "\n",
        "for idx, param_set in enumerate(param_grid):\n",
        "    print(f\"Printing numbers for param_set #{idx+1}:\")\n",
        "    news_using = param_set['news_using']\n",
        "    batch_size = param_set['batch_size']\n",
        "    num_layers = param_set['num_layers']\n",
        "    neurons = param_set['neurons']\n",
        "    epochs = param_set['epochs']\n",
        "    nmbr_folds = param_set['nmbr_folds']\n",
        "    lookback_period = param_set['lookback_period']\n",
        "    # attention_layer = param_set['attention_layer']\n",
        "    print(param_set)\n",
        "\n",
        "    ## Historical price [Y]\n",
        "\n",
        "    # 1) Reshape price, another name variable not to influence stacking dimensions below\n",
        "    #maybe as float 32\n",
        "    reshaped_price = np.reshape(adj_close, (-1, 1))\n",
        "    # 2) Train-test split - I split before Minmax scaling! GREAT\n",
        "    train_data, test_data = train_test_split(reshaped_price, test_size=test_size_split, shuffle=False)\n",
        "    # 3) Normalize data\n",
        "    scaler = MinMaxScaler()\n",
        "    train_data = scaler.fit_transform(train_data)\n",
        "    test_data = scaler.transform(test_data)\n",
        "    # 4) lookback_period dataset\n",
        "    train_X, train_Y = create_dataset(train_data, lookback_period)\n",
        "    test_X, test_Y = create_dataset(test_data, lookback_period) \n",
        "\n",
        "    ##Stacked data [X]\n",
        "\n",
        "    # scaler here more correct and does not change anything\n",
        "    scaler_sentiment = MinMaxScaler()\n",
        "    sentiment_data_scaled = sentiment_data.reshape(-1, 1)\n",
        "    sentiment_data_scaled = scaler_sentiment.fit_transform(sentiment_data_scaled)\n",
        "\n",
        "    scaler_price = MinMaxScaler()\n",
        "    adj_close_scaled = adj_close.reshape(-1, 1)\n",
        "    adj_close_scaled = scaler_price.fit_transform(adj_close_scaled)\n",
        "\n",
        "    # 1) stack data\n",
        "    if news_using == 'yes':\n",
        "      data_stacked = np.column_stack([adj_close_scaled,sentiment_data_scaled])\n",
        "      second_dimension = data_stacked.shape[1]\n",
        "    else:\n",
        "      data_stacked = np.column_stack([adj_close_scaled])\n",
        "      second_dimension = data_stacked.shape[1]  \n",
        "\n",
        "    # 2) Train-test split\n",
        "    train_stacked, test_stacked = train_test_split(data_stacked, test_size=test_size_split, shuffle=False)\n",
        "\n",
        "    # 4) Create lookback_period dataset\n",
        "    train_stacked_X, train_stacked_Y = create_dataset(train_stacked, lookback_period)\n",
        "    test_stacked_X, test_stacked_Y = create_dataset(test_stacked, lookback_period)\n",
        "\n",
        "    # Define your LSTM model\n",
        "    reset_seeds()\n",
        "    model = create_model(neurons=neurons, num_layers=num_layers, lookback=lookback_period, dimension = second_dimension) #attention_layer=attention_layer,\n",
        "\n",
        "    # Define your TimeSeriesSplit object\n",
        "    tscv = TimeSeriesSplit(n_splits=nmbr_folds)\n",
        "\n",
        "    # Loop through the splits and fit the model\n",
        "    for train_index, test_index in tscv.split(train_stacked_X):\n",
        "        \n",
        "        # Split the data into train and test sets based on the TimeSeriesSplit\n",
        "        train_stacked_X_fold, test_stacked_X_fold = train_stacked_X[train_index], train_stacked_X[test_index]\n",
        "        train_Y_fold, test_Y_fold = train_Y[train_index], train_Y[test_index]\n",
        "        \n",
        "        # fit the model\n",
        "        #model.summary\n",
        "        earlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=2, mode='min') \n",
        "        model.fit(train_stacked_X_fold, train_Y_fold, epochs=epochs, batch_size=batch_size, verbose=0, validation_split=0.1, callbacks=[earlystop])\n",
        "\n",
        "    #predictions model on test set \n",
        "    test_set_predictions = model.predict(test_stacked_X)\n",
        "    unscaled_test_set_predictions = scaler.inverse_transform(test_set_predictions)\n",
        "    unscaled_test_Y = scaler.inverse_transform(test_Y)\n",
        "\n",
        "    #metrics on test set\n",
        "    RMSE = math.sqrt(mean_squared_error(unscaled_test_Y, unscaled_test_set_predictions))\n",
        "    MAE = mean_absolute_error(unscaled_test_Y, unscaled_test_set_predictions)\n",
        "    price_unscaled_test_Y_arr = np.array(unscaled_test_Y)\n",
        "    unscaled_test_set_predictions_arr = np.array(unscaled_test_set_predictions)\n",
        "    MAPE = np.mean(np.abs((price_unscaled_test_Y_arr - unscaled_test_set_predictions_arr) / price_unscaled_test_Y_arr)) * 100\n",
        "    R2 = r2_score(unscaled_test_Y, unscaled_test_set_predictions)\n",
        "\n",
        "    print(\"RMSE: {}, MAE: {}, MAPE: {}, R2: {}\".format(RMSE, MAE, MAPE, R2))\n",
        "\n",
        "    row_data = {\n",
        "    'News': news_using,\n",
        "    'RMSE': RMSE,\n",
        "    'MAPE': MAPE,\n",
        "    'MAE': MAE,\n",
        "    'R2': R2,\n",
        "    'Batch_size': batch_size,\n",
        "    'Initiated_epochs': epochs,\n",
        "    'Lookback': lookback_period,\n",
        "    'Num_layers': num_layers,\n",
        "    'Num_neurons': neurons,\n",
        "    'Test_size_split': test_size_split,\n",
        "    'Num_folds': nmbr_folds,\n",
        "    'Set_up': set_up,\n",
        "    }\n",
        "    new_row = pd.DataFrame(row_data, index=[0])\n",
        "    results = pd.concat([results, new_row], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save df\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "results.to_csv('/content/drive/My Drive/Colab Notebooks/Official/Excel/resultsoutput_1.csv', index=False)"
      ],
      "metadata": {
        "id": "SM4D1-uF1bYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO5D72gga2zA"
      },
      "outputs": [],
      "source": [
        "# Set the figure size and DPI\n",
        "fig, ax = plt.subplots(figsize=(14, 11), dpi=80)\n",
        "\n",
        "# Generate the plot lines\n",
        "x = np.linspace(1, len(unscaled_test_set_predictions) + 1, len(unscaled_test_set_predictions), endpoint=True)\n",
        "predicted_line, = ax.plot(x, unscaled_test_set_predictions, label=\"Predicted Value\")\n",
        "actual_line, = ax.plot(x, unscaled_test_Y, label=\"Actual Value\") # or test_Y_fold\n",
        "\n",
        "# Set the title and axis labels\n",
        "ax.set_title('Prediction vs. Actual Values')\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Value')\n",
        "\n",
        "# Add the legend\n",
        "ax.legend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pllh1OeqbMrk",
        "aKFa13AOSx5I",
        "HJ-Zm5Vcqunr",
        "I6N3R94sS_Sv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}